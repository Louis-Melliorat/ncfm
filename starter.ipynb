{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Nature Conservancy Fisheries Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import pylab as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import log_loss\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_im_cv2(path):\n",
    "    img = cv2.imread(path)\n",
    "    resized = cv2.resize(img, (48, 48), interpolation = cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def load_train():\n",
    "    X_train = []\n",
    "    X_train_id = []\n",
    "    y_train = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Read train images')\n",
    "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "    for fld in folders:\n",
    "        index = folders.index(fld)\n",
    "        print('Load folder {} (Index: {})'.format(fld, index))\n",
    "        path = os.path.join('..', '/notebooks/notebooks/Fish classification/', 'train', fld, '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_im_cv2(fl)\n",
    "            X_train.append(img)\n",
    "            X_train_id.append(flbase)\n",
    "            y_train.append(index)\n",
    "     \n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return X_train, y_train, X_train_id\n",
    "\n",
    "\n",
    "def load_test():\n",
    "    path = os.path.join('..', '/notebooks/notebooks/Fish classification/', 'test_stg1', '*.jpg')\n",
    "    files = sorted(glob.glob(path))\n",
    "\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im_cv2(fl)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "\n",
    "    return X_test, X_test_id\n",
    "\n",
    "\n",
    "def split_train_test(train_features, train_labels):\n",
    "    np.random.seed(2017)\n",
    "    rand = np.random.choice(train_target.shape[0], train_target.shape[0])\n",
    "\n",
    "    #X_train_id = []\n",
    "    #X_test_id = []\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    for i in rand[0:2840]:\n",
    "        #X_train_id.append(train_id[i])\n",
    "        X_train.append(train_features[i])\n",
    "        Y_train.append(train_target[i])\n",
    "\n",
    "    for i in rand[2840:len(rand)]:\n",
    "        #X_test_id.append(train_id[i])\n",
    "        X_test.append(train_features[i])\n",
    "        Y_test.append(train_target[i]) \n",
    "       \n",
    "    print('Train set: {} images {}'.format(len(X_train),X_train[0].shape))\n",
    "    print('Test set: {} images {}'.format(len(X_test),X_test[0].shape))\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "def next_batch(batch_size,X,Y):\n",
    "    \n",
    "    idx = np.random.permutation(len(X))\n",
    "    X_shuffle =[X[i] for i in idx]\n",
    "    Y_shuffle =[Y[i] for i in idx]\n",
    "    \n",
    "    X_batch = X_shuffle[0:batch_size]\n",
    "    Y_batch = Y_shuffle[0:batch_size] \n",
    "    \n",
    "    return X_batch, Y_batch\n",
    "\n",
    "\n",
    "def create_submission(predictions, ID, name):\n",
    "    sub = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n",
    "    sub.loc[:, 'image'] = pd.Series(ID, index=sub.index)\n",
    "    sub.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder ALB (Index: 0)\n",
      "Load folder BET (Index: 1)\n",
      "Load folder DOL (Index: 2)\n",
      "Load folder LAG (Index: 3)\n",
      "Load folder NoF (Index: 4)\n",
      "Load folder OTHER (Index: 5)\n",
      "Load folder SHARK (Index: 6)\n",
      "Load folder YFT (Index: 7)\n",
      "Read train data time: 62.44 seconds\n"
     ]
    }
   ],
   "source": [
    "train_features, train_target, train_id = load_train()\n",
    "train_target = pd.get_dummies(train_target).as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data OR load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = split_train_test(train_features, train_target)\n",
    "#test_features, test_id = load_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 48, 48, 3])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 8])\n",
    "\n",
    "# 3 convolutional layers \n",
    "K = 4  # first \n",
    "L = 8  # second \n",
    "M = 12  # third \n",
    "N = 200  # fully connected layer\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 3, K], stddev=0.1))  # 5x5 patch, 1 input channel, K output channels\n",
    "B1 = tf.Variable(tf.ones([K])/8)\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([L])/8)\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([M])/8)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([12 * 12 * M, N], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([N])/8)\n",
    "W5 = tf.Variable(tf.truncated_normal([N, 8], stddev=0.1))\n",
    "B5 = tf.Variable(tf.ones([8])/8)\n",
    "\n",
    "\n",
    "# The model\n",
    "stride = 1  # output is 48*48\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "stride = 2  # output is 24*24\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "stride = 2  # output is 12*12\n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 12 * 12 * M])\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "Ylogits = tf.matmul(Y4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "# Entropy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(Ylogits, Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# accuracy \n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                    \n",
    "# Log loss\n",
    "logloss = tf.contrib.losses.log_loss(Y, Y_)\n",
    "loss = -tf.reduce_sum(Y_ * tf.log(Y)) / 937\n",
    "\n",
    "# training step\n",
    "learning_rate = 0.05\n",
    "\n",
    "# training step, the learning rate is a placeholder\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# init\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tensorflow : Train / Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "('accuracy test', 0.43970117)\n",
      "('logloss test', 0.31252381)\n",
      "('loss test', 1.6514047)\n"
     ]
    }
   ],
   "source": [
    "ITERATION = 100\n",
    "DISPLAY = True\n",
    "DISPLAY_ITER = 10 \n",
    "DISPLAY_STEP = ITERATION / DISPLAY_ITER\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Training\n",
    "for iteration in range(ITERATION):\n",
    "    batch_X, batch_Y = next_batch(BATCH_SIZE, X_train,Y_train)    \n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "    \n",
    "    if DISPLAY and (iteration % DISPLAY_STEP == 0 and iteration != 0):\n",
    "        print(iteration)\n",
    " \n",
    "print('accuracy test',sess.run(accuracy, feed_dict={X: X_test, Y_: Y_test}))\n",
    "print('logloss test',sess.run(logloss, feed_dict={X: X_test, Y_: Y_test}))\n",
    "output = sess.run(Y, feed_dict={X: X_test})\n",
    "print('loss test',sess.run(loss, feed_dict={X: X_test, Y_: Y_test}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tensorflow : All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "('accuracy test', 0.45512313)\n",
      "('logloss test', 0.30905148)\n",
      "('loss test', 6.5570149)\n"
     ]
    }
   ],
   "source": [
    "ITERATION = 100\n",
    "DISPLAY = True\n",
    "DISPLAY_ITER = 10 \n",
    "DISPLAY_STEP = ITERATION / DISPLAY_ITER\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Training\n",
    "for iteration in range(ITERATION):\n",
    "    batch_X, batch_Y = next_batch(BATCH_SIZE, train_features,train_target)    \n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "    \n",
    "    if DISPLAY and (iteration % DISPLAY_STEP == 0 and iteration != 0):\n",
    "        print(iteration)\n",
    " \n",
    "print('accuracy test',sess.run(accuracy, feed_dict={X: train_features, Y_: train_target}))\n",
    "print('logloss test',sess.run(logloss, feed_dict={X: train_features, Y_: train_target}))\n",
    "# test_features, test_id\n",
    "output = sess.run(Y, feed_dict={X: test_features})\n",
    "print('loss test',sess.run(loss, feed_dict={X: train_features, Y_: train_target}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission(output,test_id,'sub1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
